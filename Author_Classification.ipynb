{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writer Verification Project\n",
    "\n",
    "## Description:\n",
    "This project is divided in two parts:\n",
    "1. Data Pre-processing \n",
    "2. Training Model on convNET model(AlexNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will import all the necessary libraries for this project. For any python program we can get different libraries that make our tasks easier. You can get more details on each of these libraries in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python==4.1.0.25\n",
    "# !pip install imutils\n",
    "# !pip install pandas==0.23.4\n",
    "# !pip install tensorflow==1.14.0\n",
    "# !pip install keras==2.2.4\n",
    "# !pip install split_folders\n",
    "# !pip install matplotlib==3.0.2\n",
    "# !pip install Pillow==5.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import csv\n",
    "import math\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import imutils\n",
    "import numpy as np\n",
    "import splitfolders\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import MaxPooling2D, Conv2D, Reshape, Dense, Input, Flatten, GlobalAveragePooling2D,Convolution2D, Activation, Dropout\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation:\n",
    "\n",
    "For this part we need to acquire pathches from the images of each author so that we can train our algorithm to recognize the work/writing pattern of the author.\n",
    "\n",
    "For this we will use image opencv to detect contours in the image & copy them to a new file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the next cell I have done some image processing tasks so that only the writings are visible for making patches.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = cv2.imread(\"raw_data/1/image6.jpg\")\n",
    "image = cv2.resize(pic,(int(pic.shape[1]/2),int(pic.shape[0]/2))) \n",
    "cv2.imshow(\"Orignal Image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "# Converting image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# Converting it to binary mapping\n",
    "ret,thresh1 = cv2.threshold(gray,50,255,cv2.THRESH_BINARY_INV)\n",
    "# Applying morphology\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "closing = cv2.morphologyEx(thresh1, cv2.MORPH_CLOSE, kernel)\n",
    "# Resulting Image\n",
    "cv2.imshow(\"Final Image\", thresh1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we get that value we will extract the area only with some writing in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours, hierarchy = cv2.findContours(closing, 1, 3)\n",
    "idx = 0\n",
    "x,y,w,h = (2000,2000,0,0)\n",
    "for cnt in contours:\n",
    "    idx += 1\n",
    "    x_x, y_y, w_w, h_h = cv2.boundingRect(cnt)\n",
    "    if x_x<x:\n",
    "        x=x_x\n",
    "    if y_y<y:\n",
    "        y=y_y\n",
    "    if x_x>w:\n",
    "        w=x_x\n",
    "    if y_y>h:\n",
    "        h=y_y\n",
    "crop_img = image[y:h, x:w]\n",
    "crop_img_inv = thresh1[y:h, x:w]\n",
    "cv2.imshow(\"Final Image\", crop_img_inv)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to extract patches of same dimensions so we this code will do.\n",
    "\n",
    "**NOTE**\n",
    "1. We will also add a check if the intensity is less than so don't include that patch as it will not have much values in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dimx,dimy,c) = crop_img.shape\n",
    "div = 10\n",
    "if crop_img.shape[0]>crop_img.shape[1]:\n",
    "    patch_shape = (int(dimx/div),int(dimx/div))\n",
    "    stride = (div*crop_img.shape[1]/int(dimx/div))\n",
    "else:\n",
    "    patch_shape = (int(dimy/div),int(dimy/div))\n",
    "    stride = (div*crop_img.shape[0]/int(dimy/div))\n",
    "s_path = \"results\"\n",
    "if not os.path.exists(s_path):\n",
    "    os.makedirs(s_path)\n",
    "def patchify(img, patch_shape):\n",
    "    img = np.ascontiguousarray(img)  # won't make a copy if not needed\n",
    "    X, Y, C = img.shape\n",
    "    h, w = patch_shape\n",
    "    x,y = 0,0\n",
    "    for i in range(0,200):\n",
    "        crop = crop_img[y:y+h, x:x+w]\n",
    "        crop_inv = crop_img_inv[y:y+h, x:x+w]\n",
    "        x += h\n",
    "        Intensity = np.average(crop_inv)\n",
    "        if Intensity<6:\n",
    "            continue\n",
    "        if (crop.shape!=(h,w,C)):\n",
    "            x = 0\n",
    "            y += h\n",
    "            continue\n",
    "        try:\n",
    "            crop = cv2.resize(crop,(224,224))\n",
    "            cv2.imwrite(s_path + \"/{}.jpg\".format(i),crop)\n",
    "        except:\n",
    "            x = 0\n",
    "            y += h\n",
    "            continue\n",
    "    \n",
    "    return\n",
    "patches = patchify(crop_img,patch_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will add all these functionalities to function that will do all that task. But first let us see how can we access all these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(crop_img, crop_img_inv, patch_shape,subdir,file):\n",
    "    save_in = s_path+\"/\"+subdir\n",
    "    if not os.path.exists(save_in):\n",
    "        os.makedirs(save_in)\n",
    "    img = np.ascontiguousarray(crop_img)  # won't make a copy if not needed\n",
    "    X, Y, C = img.shape\n",
    "    h, w = patch_shape\n",
    "    x,y = 0,0\n",
    "    for i in range(0,200):\n",
    "        crop = crop_img[y:y+h, x:x+w]\n",
    "        crop_inv = crop_img_inv[y:y+h, x:x+w]\n",
    "        x += h\n",
    "        Intensity = np.average(crop_inv)\n",
    "        if Intensity<6:\n",
    "            continue\n",
    "        if (crop.shape!=(h,w,C)):\n",
    "            x = 0\n",
    "            y += h\n",
    "            continue\n",
    "        try:\n",
    "            crop = cv2.resize(crop,(224,224))\n",
    "            cv2.imwrite(save_in + \"/{}_{}_{}.jpg\".format(subdir,file,i),crop)\n",
    "        except:\n",
    "            x = 0\n",
    "            y += h\n",
    "            continue\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patches_extraction(path,s_path):\n",
    "    for subdir in os.listdir(path):\n",
    "        print(\"The directory name is: \",subdir)\n",
    "        name = 0\n",
    "        for file in os.listdir(path+\"/\"+ subdir):\n",
    "            print(\"The file name is: \",file)\n",
    "            name += 1\n",
    "            if not file.endswith(\".jpg\"):\n",
    "                continue\n",
    "            pic = cv2.imread(path+\"/\"+ subdir+\"/\"+file)\n",
    "            image = cv2.resize(pic,(int(pic.shape[1]/2),int(pic.shape[0]/2))) \n",
    "            # cv2.imshow(\"Orignal Image\", image)\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyAllWindows()\n",
    "            # Converting image to grayscale\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            # Converting it to binary mapping\n",
    "            ret,thresh1 = cv2.threshold(gray,50,255,cv2.THRESH_BINARY_INV)\n",
    "            # Applying morphology\n",
    "            kernel = np.ones((5,5),np.uint8)\n",
    "            closing = cv2.morphologyEx(thresh1, cv2.MORPH_CLOSE, kernel)\n",
    "            contours, hierarchy = cv2.findContours(closing, 1, 3)\n",
    "            idx = 0\n",
    "            x,y,w,h = (2000,2000,0,0)\n",
    "            for cnt in contours:\n",
    "                idx += 1\n",
    "                x_x, y_y, w_w, h_h = cv2.boundingRect(cnt)\n",
    "                if x_x<x:\n",
    "                    x=x_x\n",
    "                if y_y<y:\n",
    "                    y=y_y\n",
    "                if x_x>w:\n",
    "                    w=x_x\n",
    "                if y_y>h:\n",
    "                    h=y_y\n",
    "            crop_img = image[y:h, x:w]\n",
    "            crop_img_inv = thresh1[y:h, x:w]\n",
    "            (dimx,dimy,c) = crop_img.shape\n",
    "            div = 10\n",
    "            if crop_img.shape[0]>crop_img.shape[1]:\n",
    "                patch_shape = (int(dimx/div),int(dimx/div))\n",
    "            else:\n",
    "                patch_shape = (int(dimy/div),int(dimy/div))\n",
    "            patches = patchify(crop_img, crop_img_inv,patch_shape,subdir,name)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will gonna run this on the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = (r\"C:\\Users\\NE\\Author-Classification-of-Arabic-Scripts-Using-CNN\\raw_data\")\n",
    "s_path = (r\"C:\\Users\\NE\\Author-Classification-of-Arabic-Scripts-Using-CNN\\processed_data\")\n",
    "patches_extraction(path,s_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split our data into train & test dataset. We will use the library \"https://pypi.org/project/split-folders/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_path = (r\"C:\\Users\\NE\\Author-Classification-of-Arabic-Scripts-Using-CNN\\processed_data\")\n",
    "split_path = (r\"C:\\Users\\NE\\Author-Classification-of-Arabic-Scripts-Using-CNN\\split_data\")\n",
    "if not os.path.exists(split_path):\n",
    "        os.makedirs(split_path)\n",
    "splitfolders.ratio(s_path, output=split_path, seed=1337, ratio=(.8, .1, .1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is all set for training! :) We will now make the model.\n",
    "In this cell we initialize some parameters that will be given to our model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_data_path = r'C:\\Users\\NE\\Author-Classification-of-Arabic-Scripts-Using-CNN\\split_data\\train'\n",
    "validation_data_path = r'C:\\Users\\NE\\Author-Classification-of-Arabic-Scripts-Using-CNN\\split_data\\val'\n",
    "test_data_path = r'C:\\Users\\NE\\Author-Classification-of-Arabic-Scripts-Using-CNN\\split_data\\test'\n",
    "\n",
    "\"\"\"\n",
    "Parameters\n",
    "\"\"\"\n",
    "epochs = 10\n",
    "lr = 0.0004\n",
    "\n",
    "'''\n",
    "Call Backs\n",
    "'''\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\",\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet Model\n",
    "\n",
    "Now as discussed in paper they used an AlexNet model for training so here is the model. You can compare the model summary with that in the paper & it is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "#Instantiate an empty model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "# Passing it to a Fully Connected layer\n",
    "model.add(Flatten())\n",
    "# 1st Fully Connected Layer\n",
    "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# 2nd Fully Connected Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# 3rd Fully Connected Layer\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will initialize our generators. Generators help us get data as in from all the different folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./1,\n",
    "    horizontal_flip=False)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=6,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=6,\n",
    "    class_mode=\"categorical\")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_path,\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=1,\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to find the step size we use our generators & batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now we will start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "sys.modules['Image'] = Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=20,\n",
    "                    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "Since we are evaluating the model, we should treat the validation set as if it was the test set. So we should sample the images in the validation set exactly once(if you are planning to evaluate, you need to change the batch size of the valid generator to 1 or something that exactly divides the total num of samples in validation set), but the order doesn’t matter so let “shuffle” be True as it was earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(generator=validation_generator,\n",
    "steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting from the model\n",
    "\n",
    "You need to reset the test_generator before whenever you call the predict_generator. This is important, if you forget to reset the test_generator you will get outputs in a weird order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict_generator(test_generator,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now predicted_class_indices has the predicted labels, but you can’t simply tell what the predictions are, because all you can see is numbers like 0,1,4,1,0,6…\n",
    "You need to map the predicted labels with their unique ids such as filenames to find out what you predicted for which image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices = np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save the results as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "results.to_csv(\"results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
